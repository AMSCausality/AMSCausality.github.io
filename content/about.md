---
title: "About"
---

# About the workshop

Current machine learning systems have rapidly increased in performance by leveraging ever-larger models and datasets. Despite astonishing abilities and impressive demos, these models fundamentally only learn from statistical correlations and struggle at tasks such as domain generalisation, adversarial examples, or planning, which require higher-order cognition. This sole reliance on capturing correlations sits at the core of current debates about making AI systems ``truly'' understand. One promising and so far underexplored approach for obtaining visual systems that can go beyond correlations is integrating ideas from causality into representation learning.

Causal inference aims to reason about the effect of interventions or external manipulations on a system, as well as about hypothetical counterfactual scenarios. Similar to classic approaches to AI, it typically assumes that the causal variables of interest are given from the outset. However, real-world data often comprises high-dimensional, low-level observations (e.g., RGB pixels in a video) and is thus usually not structured into such meaningful causal units. 

To this end, the emerging field of causal representation learning (CRL) combines the strengths of ML and causality. In CRL we aim at learning low-dimensional, high-level causal variables along with their causal relations directly from raw, unstructured data, leading to representations that support notions such as causal factors, intervention, reasoning, and planning. In this sense, CRL aligns with the general goal of modern ML to learn meaningful representations of data that are more robust, explainable, and performant, and in our workshop we want to catalyze research in this direction.

This workshop brings together researchers from the emerging CRL community, as well as from the more classical causality and representation learning communities, who are interested in learning causal, robust, interpretable and transferrable representations. Our goal is to foster discussion and cross-fertilization between causality, representation learning and other fields, as well as to engage the community in identifying application domains for this emerging new field. 

<!-- Machine learning (ML) has established itself as the dominant and most successful paradigm for artificial intelligence (AI). A key strength of ML over earlier (symbolic, logic and rule-based) approaches to AI, is its ability to infer useful features or *representations* of often very high-dimensional observations in an automated, data-driven way. However, in doing so, it generally only leverages *statistical* information (e.g., correlations present in a training set) and consequently struggles at tasks such as knowledge transfer, systematic generalization, or planning, which are thought to require higher-order cognition.

Causal inference (CI), on the other hand, is concerned with going beyond the statistical level of description (“seeing”) and instead aims to reason about the effect of interventions or external manipulations to a system (“doing”) as well as about hypothetical counterfactual scenarios (“imagining”). Similar to classic approaches to AI, CI typically assumes that the causal variables of interest (i.e., an appropriate level of description of a given system) are given from the outset. However, real-world data often comprises high-dimensional, low-level observations and is thus usually not structured into such meaningful causal units. 

The emerging field of causal representation learning (CRL) aims to combine the strengths of ML and CI. Much like ML went beyond symbolic AI in not requiring that the symbols that algorithms manipulate be given a priori, in CRL low-dimensional, high-level variables *along with their causal relations* should be learned from raw, unstructured data, leading to representations that support notions such as intervention, reasoning, and planning. In this sense, CRL aligns with the general goal of modern ML to learn *meaningful* representations of data, where meaningful can also include *robust, explainable,* or *fair*.

One aim of this first workshop on CRL is to bring together researchers focusing mainly on either CI or representation learning, from both theoretical and applied perspectives. Moreover, the workshop aims at engaging the various communities interested in learning robust and transferable representations from different perspectives, in order to foster an exchange of ideas. Given that this is still a young, emerging line of research, another goal is to establish a common vocabulary and to identify useful frameworks for addressing CRL.  -->
